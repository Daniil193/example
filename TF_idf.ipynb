{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Import lib and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import make_union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('/home/dan/Test/Kaggle/data/TF_idf/train.csv').fillna(' ')\n",
    "test = pd.read_csv('/home/dan/Test/Kaggle/data/TF_idf/test.csv').fillna(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class_names = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "train_text = train['comment_text']\n",
    "test_text = test['comment_text']\n",
    "all_text = pd.concat([train_text, test_text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='word',\n",
    "    token_pattern=r'\\w{1,}',\n",
    "    ngram_range=(1, 1),\n",
    "    max_features=30000)\n",
    "char_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='char',\n",
    "    ngram_range=(1, 4),\n",
    "    max_features=30000)\n",
    "vectorizer = make_union(word_vectorizer, char_vectorizer, n_jobs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "JoblibMemoryError",
     "evalue": "JoblibMemoryError\n___________________________________________________________________________\nMultiprocessing exception:\n...........................................................................\n/home/dan/anaconda3/lib/python3.6/runpy.py in _run_module_as_main(mod_name='ipykernel_launcher', alter_argv=1)\n    188         sys.exit(msg)\n    189     main_globals = sys.modules[\"__main__\"].__dict__\n    190     if alter_argv:\n    191         sys.argv[0] = mod_spec.origin\n    192     return _run_code(code, main_globals, None,\n--> 193                      \"__main__\", mod_spec)\n        mod_spec = ModuleSpec(name='ipykernel_launcher', loader=<_f...b/python3.6/site-packages/ipykernel_launcher.py')\n    194 \n    195 def run_module(mod_name, init_globals=None,\n    196                run_name=None, alter_sys=False):\n    197     \"\"\"Execute a module's code without importing it\n\n...........................................................................\n/home/dan/anaconda3/lib/python3.6/runpy.py in _run_code(code=<code object <module> at 0x7f51e67915d0, file \"/...3.6/site-packages/ipykernel_launcher.py\", line 5>, run_globals={'__annotations__': {}, '__builtins__': <module 'builtins' (built-in)>, '__cached__': '/home/dan/anaconda3/lib/python3.6/site-packages/__pycache__/ipykernel_launcher.cpython-36.pyc', '__doc__': 'Entry point for launching an IPython kernel.\\n\\nTh...orts until\\nafter removing the cwd from sys.path.\\n', '__file__': '/home/dan/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py', '__loader__': <_frozen_importlib_external.SourceFileLoader object>, '__name__': '__main__', '__package__': '', '__spec__': ModuleSpec(name='ipykernel_launcher', loader=<_f...b/python3.6/site-packages/ipykernel_launcher.py'), 'app': <module 'ipykernel.kernelapp' from '/home/dan/an.../python3.6/site-packages/ipykernel/kernelapp.py'>, ...}, init_globals=None, mod_name='__main__', mod_spec=ModuleSpec(name='ipykernel_launcher', loader=<_f...b/python3.6/site-packages/ipykernel_launcher.py'), pkg_name='', script_name=None)\n     80                        __cached__ = cached,\n     81                        __doc__ = None,\n     82                        __loader__ = loader,\n     83                        __package__ = pkg_name,\n     84                        __spec__ = mod_spec)\n---> 85     exec(code, run_globals)\n        code = <code object <module> at 0x7f51e67915d0, file \"/...3.6/site-packages/ipykernel_launcher.py\", line 5>\n        run_globals = {'__annotations__': {}, '__builtins__': <module 'builtins' (built-in)>, '__cached__': '/home/dan/anaconda3/lib/python3.6/site-packages/__pycache__/ipykernel_launcher.cpython-36.pyc', '__doc__': 'Entry point for launching an IPython kernel.\\n\\nTh...orts until\\nafter removing the cwd from sys.path.\\n', '__file__': '/home/dan/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py', '__loader__': <_frozen_importlib_external.SourceFileLoader object>, '__name__': '__main__', '__package__': '', '__spec__': ModuleSpec(name='ipykernel_launcher', loader=<_f...b/python3.6/site-packages/ipykernel_launcher.py'), 'app': <module 'ipykernel.kernelapp' from '/home/dan/an.../python3.6/site-packages/ipykernel/kernelapp.py'>, ...}\n     86     return run_globals\n     87 \n     88 def _run_module_code(code, init_globals=None,\n     89                     mod_name=None, mod_spec=None,\n\n...........................................................................\n/home/dan/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py in <module>()\n     11     # This is added back by InteractiveShellApp.init_path()\n     12     if sys.path[0] == '':\n     13         del sys.path[0]\n     14 \n     15     from ipykernel import kernelapp as app\n---> 16     app.launch_new_instance()\n\n...........................................................................\n/home/dan/anaconda3/lib/python3.6/site-packages/traitlets/config/application.py in launch_instance(cls=<class 'ipykernel.kernelapp.IPKernelApp'>, argv=None, **kwargs={})\n    653 \n    654         If a global instance already exists, this reinitializes and starts it\n    655         \"\"\"\n    656         app = cls.instance(**kwargs)\n    657         app.initialize(argv)\n--> 658         app.start()\n        app.start = <bound method IPKernelApp.start of <ipykernel.kernelapp.IPKernelApp object>>\n    659 \n    660 #-----------------------------------------------------------------------------\n    661 # utility functions, for convenience\n    662 #-----------------------------------------------------------------------------\n\n...........................................................................\n/home/dan/anaconda3/lib/python3.6/site-packages/ipykernel/kernelapp.py in start(self=<ipykernel.kernelapp.IPKernelApp object>)\n    472             return self.subapp.start()\n    473         if self.poller is not None:\n    474             self.poller.start()\n    475         self.kernel.start()\n    476         try:\n--> 477             ioloop.IOLoop.instance().start()\n    478         except KeyboardInterrupt:\n    479             pass\n    480 \n    481 launch_new_instance = IPKernelApp.launch_instance\n\n...........................................................................\n/home/dan/anaconda3/lib/python3.6/site-packages/zmq/eventloop/ioloop.py in start(self=<zmq.eventloop.ioloop.ZMQIOLoop object>)\n    172             )\n    173         return loop\n    174     \n    175     def start(self):\n    176         try:\n--> 177             super(ZMQIOLoop, self).start()\n        self.start = <bound method ZMQIOLoop.start of <zmq.eventloop.ioloop.ZMQIOLoop object>>\n    178         except ZMQError as e:\n    179             if e.errno == ETERM:\n    180                 # quietly return on ETERM\n    181                 pass\n\n...........................................................................\n/home/dan/anaconda3/lib/python3.6/site-packages/tornado/ioloop.py in start(self=<zmq.eventloop.ioloop.ZMQIOLoop object>)\n    883                 self._events.update(event_pairs)\n    884                 while self._events:\n    885                     fd, events = self._events.popitem()\n    886                     try:\n    887                         fd_obj, handler_func = self._handlers[fd]\n--> 888                         handler_func(fd_obj, events)\n        handler_func = <function wrap.<locals>.null_wrapper>\n        fd_obj = <zmq.sugar.socket.Socket object>\n        events = 1\n    889                     except (OSError, IOError) as e:\n    890                         if errno_from_exception(e) == errno.EPIPE:\n    891                             # Happens when the client closes the connection\n    892                             pass\n\n...........................................................................\n/home/dan/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py in null_wrapper(*args=(<zmq.sugar.socket.Socket object>, 1), **kwargs={})\n    272         # Fast path when there are no active contexts.\n    273         def null_wrapper(*args, **kwargs):\n    274             try:\n    275                 current_state = _state.contexts\n    276                 _state.contexts = cap_contexts[0]\n--> 277                 return fn(*args, **kwargs)\n        args = (<zmq.sugar.socket.Socket object>, 1)\n        kwargs = {}\n    278             finally:\n    279                 _state.contexts = current_state\n    280         null_wrapper._wrapped = True\n    281         return null_wrapper\n\n...........................................................................\n/home/dan/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py in _handle_events(self=<zmq.eventloop.zmqstream.ZMQStream object>, fd=<zmq.sugar.socket.Socket object>, events=1)\n    435             # dispatch events:\n    436             if events & IOLoop.ERROR:\n    437                 gen_log.error(\"got POLLERR event on ZMQStream, which doesn't make sense\")\n    438                 return\n    439             if events & IOLoop.READ:\n--> 440                 self._handle_recv()\n        self._handle_recv = <bound method ZMQStream._handle_recv of <zmq.eventloop.zmqstream.ZMQStream object>>\n    441                 if not self.socket:\n    442                     return\n    443             if events & IOLoop.WRITE:\n    444                 self._handle_send()\n\n...........................................................................\n/home/dan/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py in _handle_recv(self=<zmq.eventloop.zmqstream.ZMQStream object>)\n    467                 gen_log.error(\"RECV Error: %s\"%zmq.strerror(e.errno))\n    468         else:\n    469             if self._recv_callback:\n    470                 callback = self._recv_callback\n    471                 # self._recv_callback = None\n--> 472                 self._run_callback(callback, msg)\n        self._run_callback = <bound method ZMQStream._run_callback of <zmq.eventloop.zmqstream.ZMQStream object>>\n        callback = <function wrap.<locals>.null_wrapper>\n        msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]\n    473                 \n    474         # self.update_state()\n    475         \n    476 \n\n...........................................................................\n/home/dan/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py in _run_callback(self=<zmq.eventloop.zmqstream.ZMQStream object>, callback=<function wrap.<locals>.null_wrapper>, *args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})\n    409         close our socket.\"\"\"\n    410         try:\n    411             # Use a NullContext to ensure that all StackContexts are run\n    412             # inside our blanket exception handler rather than outside.\n    413             with stack_context.NullContext():\n--> 414                 callback(*args, **kwargs)\n        callback = <function wrap.<locals>.null_wrapper>\n        args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)\n        kwargs = {}\n    415         except:\n    416             gen_log.error(\"Uncaught exception, closing connection.\",\n    417                           exc_info=True)\n    418             # Close the socket on an uncaught exception from a user callback\n\n...........................................................................\n/home/dan/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py in null_wrapper(*args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})\n    272         # Fast path when there are no active contexts.\n    273         def null_wrapper(*args, **kwargs):\n    274             try:\n    275                 current_state = _state.contexts\n    276                 _state.contexts = cap_contexts[0]\n--> 277                 return fn(*args, **kwargs)\n        args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)\n        kwargs = {}\n    278             finally:\n    279                 _state.contexts = current_state\n    280         null_wrapper._wrapped = True\n    281         return null_wrapper\n\n...........................................................................\n/home/dan/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py in dispatcher(msg=[<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>])\n    278         if self.control_stream:\n    279             self.control_stream.on_recv(self.dispatch_control, copy=False)\n    280 \n    281         def make_dispatcher(stream):\n    282             def dispatcher(msg):\n--> 283                 return self.dispatch_shell(stream, msg)\n        msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]\n    284             return dispatcher\n    285 \n    286         for s in self.shell_streams:\n    287             s.on_recv(make_dispatcher(s), copy=False)\n\n...........................................................................\n/home/dan/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py in dispatch_shell(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, msg={'buffers': [], 'content': {'allow_stdin': True, 'code': 'vectorizer.fit(all_text)\\ntrain_features = vector...)\\ntest_features = vectorizer.transform(test_text)', 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2018, 8, 8, 10, 29, 39, 451027, tzinfo=tzutc()), 'msg_id': '0A57D0D6673042B2AA1F61ADFF6B2C96', 'msg_type': 'execute_request', 'session': '17FB4326E53D41888A6060F835E8A03C', 'username': 'username', 'version': '5.0'}, 'metadata': {}, 'msg_id': '0A57D0D6673042B2AA1F61ADFF6B2C96', 'msg_type': 'execute_request', 'parent_header': {}})\n    230             self.log.warn(\"Unknown message type: %r\", msg_type)\n    231         else:\n    232             self.log.debug(\"%s: %s\", msg_type, msg)\n    233             self.pre_handler_hook()\n    234             try:\n--> 235                 handler(stream, idents, msg)\n        handler = <bound method Kernel.execute_request of <ipykernel.ipkernel.IPythonKernel object>>\n        stream = <zmq.eventloop.zmqstream.ZMQStream object>\n        idents = [b'17FB4326E53D41888A6060F835E8A03C']\n        msg = {'buffers': [], 'content': {'allow_stdin': True, 'code': 'vectorizer.fit(all_text)\\ntrain_features = vector...)\\ntest_features = vectorizer.transform(test_text)', 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2018, 8, 8, 10, 29, 39, 451027, tzinfo=tzutc()), 'msg_id': '0A57D0D6673042B2AA1F61ADFF6B2C96', 'msg_type': 'execute_request', 'session': '17FB4326E53D41888A6060F835E8A03C', 'username': 'username', 'version': '5.0'}, 'metadata': {}, 'msg_id': '0A57D0D6673042B2AA1F61ADFF6B2C96', 'msg_type': 'execute_request', 'parent_header': {}}\n    236             except Exception:\n    237                 self.log.error(\"Exception in message handler:\", exc_info=True)\n    238             finally:\n    239                 self.post_handler_hook()\n\n...........................................................................\n/home/dan/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py in execute_request(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, ident=[b'17FB4326E53D41888A6060F835E8A03C'], parent={'buffers': [], 'content': {'allow_stdin': True, 'code': 'vectorizer.fit(all_text)\\ntrain_features = vector...)\\ntest_features = vectorizer.transform(test_text)', 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2018, 8, 8, 10, 29, 39, 451027, tzinfo=tzutc()), 'msg_id': '0A57D0D6673042B2AA1F61ADFF6B2C96', 'msg_type': 'execute_request', 'session': '17FB4326E53D41888A6060F835E8A03C', 'username': 'username', 'version': '5.0'}, 'metadata': {}, 'msg_id': '0A57D0D6673042B2AA1F61ADFF6B2C96', 'msg_type': 'execute_request', 'parent_header': {}})\n    394         if not silent:\n    395             self.execution_count += 1\n    396             self._publish_execute_input(code, parent, self.execution_count)\n    397 \n    398         reply_content = self.do_execute(code, silent, store_history,\n--> 399                                         user_expressions, allow_stdin)\n        user_expressions = {}\n        allow_stdin = True\n    400 \n    401         # Flush output before sending the reply.\n    402         sys.stdout.flush()\n    403         sys.stderr.flush()\n\n...........................................................................\n/home/dan/anaconda3/lib/python3.6/site-packages/ipykernel/ipkernel.py in do_execute(self=<ipykernel.ipkernel.IPythonKernel object>, code='vectorizer.fit(all_text)\\ntrain_features = vector...)\\ntest_features = vectorizer.transform(test_text)', silent=False, store_history=True, user_expressions={}, allow_stdin=True)\n    191 \n    192         self._forward_input(allow_stdin)\n    193 \n    194         reply_content = {}\n    195         try:\n--> 196             res = shell.run_cell(code, store_history=store_history, silent=silent)\n        res = undefined\n        shell.run_cell = <bound method ZMQInteractiveShell.run_cell of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        code = 'vectorizer.fit(all_text)\\ntrain_features = vector...)\\ntest_features = vectorizer.transform(test_text)'\n        store_history = True\n        silent = False\n    197         finally:\n    198             self._restore_input()\n    199 \n    200         if res.error_before_exec is not None:\n\n...........................................................................\n/home/dan/anaconda3/lib/python3.6/site-packages/ipykernel/zmqshell.py in run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, *args=('vectorizer.fit(all_text)\\ntrain_features = vector...)\\ntest_features = vectorizer.transform(test_text)',), **kwargs={'silent': False, 'store_history': True})\n    528             )\n    529         self.payload_manager.write_payload(payload)\n    530 \n    531     def run_cell(self, *args, **kwargs):\n    532         self._last_traceback = None\n--> 533         return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n        self.run_cell = <bound method ZMQInteractiveShell.run_cell of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        args = ('vectorizer.fit(all_text)\\ntrain_features = vector...)\\ntest_features = vectorizer.transform(test_text)',)\n        kwargs = {'silent': False, 'store_history': True}\n    534 \n    535     def _showtraceback(self, etype, evalue, stb):\n    536         # try to preserve ordering of tracebacks and print statements\n    537         sys.stdout.flush()\n\n...........................................................................\n/home/dan/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py in run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, raw_cell='vectorizer.fit(all_text)\\ntrain_features = vector...)\\ntest_features = vectorizer.transform(test_text)', store_history=True, silent=False, shell_futures=True)\n   2693                 self.displayhook.exec_result = result\n   2694 \n   2695                 # Execute the user code\n   2696                 interactivity = \"none\" if silent else self.ast_node_interactivity\n   2697                 has_raised = self.run_ast_nodes(code_ast.body, cell_name,\n-> 2698                    interactivity=interactivity, compiler=compiler, result=result)\n        interactivity = 'last_expr'\n        compiler = <IPython.core.compilerop.CachingCompiler object>\n   2699                 \n   2700                 self.last_execution_succeeded = not has_raised\n   2701 \n   2702                 # Reset this so later displayed values do not modify the\n\n...........................................................................\n/home/dan/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py in run_ast_nodes(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, nodelist=[<_ast.Expr object>, <_ast.Assign object>, <_ast.Assign object>], cell_name='<ipython-input-20-2f1297c59012>', interactivity='none', compiler=<IPython.core.compilerop.CachingCompiler object>, result=<ExecutionResult object at 7f51ac53eb38, executi..._before_exec=None error_in_exec=None result=None>)\n   2797 \n   2798         try:\n   2799             for i, node in enumerate(to_run_exec):\n   2800                 mod = ast.Module([node])\n   2801                 code = compiler(mod, cell_name, \"exec\")\n-> 2802                 if self.run_code(code, result):\n        self.run_code = <bound method InteractiveShell.run_code of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        code = <code object <module> at 0x7f51a0033c90, file \"<ipython-input-20-2f1297c59012>\", line 1>\n        result = <ExecutionResult object at 7f51ac53eb38, executi..._before_exec=None error_in_exec=None result=None>\n   2803                     return True\n   2804 \n   2805             for i, node in enumerate(to_run_interactive):\n   2806                 mod = ast.Interactive([node])\n\n...........................................................................\n/home/dan/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py in run_code(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, code_obj=<code object <module> at 0x7f51a0033c90, file \"<ipython-input-20-2f1297c59012>\", line 1>, result=<ExecutionResult object at 7f51ac53eb38, executi..._before_exec=None error_in_exec=None result=None>)\n   2857         outflag = True  # happens in more places, so it's easier as default\n   2858         try:\n   2859             try:\n   2860                 self.hooks.pre_run_code_hook()\n   2861                 #rprint('Running code', repr(code_obj)) # dbg\n-> 2862                 exec(code_obj, self.user_global_ns, self.user_ns)\n        code_obj = <code object <module> at 0x7f51a0033c90, file \"<ipython-input-20-2f1297c59012>\", line 1>\n        self.user_global_ns = {'In': ['', 'import numpy as np\\nimport pandas as pd\\nfrom skle...val_score\\nfrom sklearn.pipeline import make_union', \"train = pd.read_csv('/home/dan/Test/Kaggle/data/...an/Test/Kaggle/data/TF_idf/test.csv').fillna(' ')\", 'train[0]', 'train.head()', \"train = pd.read_csv('/home/dan/Test/Kaggle/data/...an/Test/Kaggle/data/TF_idf/test.csv').fillna(' ')\", 'train.head()', \"train['comment_text'].head()\", 'train.head()', \"train = pd.read_csv('/home/dan/Test/Kaggle/data/...an/Test/Kaggle/data/TF_idf/test.csv').fillna(' ')\", 'train.head()', 'train.head()', \"train = pd.read_csv('/home/dan/Test/Kaggle/data/...an/Test/Kaggle/data/TF_idf/test.csv').fillna(' ')\", 'train.dtypes', 'train.info()', 'train.head()', \"train = pd.read_csv('/home/dan/Test/Kaggle/data/...an/Test/Kaggle/data/TF_idf/test.csv').fillna(' ')\", 'train.head()', \"class_names = ['toxic', 'severe_toxic', 'obscene...t']\\nall_text = pd.concat([train_text, test_text])\", 'word_vectorizer = TfidfVectorizer(\\n    sublinear...union(word_vectorizer, char_vectorizer, n_jobs=2)', ...], 'LogisticRegression': <class 'sklearn.linear_model.logistic.LogisticRegression'>, 'Out': {4:                  id                             ...      0        0       0       0              0  , 6:                  id                             ...      0        0       0       0              0  , 7: 0    Explanation\\nWhy the edits made under my us...you remember...\nName: comment_text, dtype: object, 8:                  id                             ...      0        0       0       0              0  , 10:                  id                             ...      0        0       0       0              0  , 11:                  id                             ...      0        0       0       0              0  , 13: id               object\ncomment_text     object\n...      int64\nidentity_hate     int64\ndtype: object, 15:                  id                             ...      0        0       0       0              0  , 17:                  id                             ...      0        0       0       0              0  }, 'TfidfVectorizer': <class 'sklearn.feature_extraction.text.TfidfVectorizer'>, '_':                  id                             ...      0        0       0       0              0  , '_10':                  id                             ...      0        0       0       0              0  , '_11':                  id                             ...      0        0       0       0              0  , '_13': id               object\ncomment_text     object\n...      int64\nidentity_hate     int64\ndtype: object, '_15':                  id                             ...      0        0       0       0              0  , '_17':                  id                             ...      0        0       0       0              0  , ...}\n        self.user_ns = {'In': ['', 'import numpy as np\\nimport pandas as pd\\nfrom skle...val_score\\nfrom sklearn.pipeline import make_union', \"train = pd.read_csv('/home/dan/Test/Kaggle/data/...an/Test/Kaggle/data/TF_idf/test.csv').fillna(' ')\", 'train[0]', 'train.head()', \"train = pd.read_csv('/home/dan/Test/Kaggle/data/...an/Test/Kaggle/data/TF_idf/test.csv').fillna(' ')\", 'train.head()', \"train['comment_text'].head()\", 'train.head()', \"train = pd.read_csv('/home/dan/Test/Kaggle/data/...an/Test/Kaggle/data/TF_idf/test.csv').fillna(' ')\", 'train.head()', 'train.head()', \"train = pd.read_csv('/home/dan/Test/Kaggle/data/...an/Test/Kaggle/data/TF_idf/test.csv').fillna(' ')\", 'train.dtypes', 'train.info()', 'train.head()', \"train = pd.read_csv('/home/dan/Test/Kaggle/data/...an/Test/Kaggle/data/TF_idf/test.csv').fillna(' ')\", 'train.head()', \"class_names = ['toxic', 'severe_toxic', 'obscene...t']\\nall_text = pd.concat([train_text, test_text])\", 'word_vectorizer = TfidfVectorizer(\\n    sublinear...union(word_vectorizer, char_vectorizer, n_jobs=2)', ...], 'LogisticRegression': <class 'sklearn.linear_model.logistic.LogisticRegression'>, 'Out': {4:                  id                             ...      0        0       0       0              0  , 6:                  id                             ...      0        0       0       0              0  , 7: 0    Explanation\\nWhy the edits made under my us...you remember...\nName: comment_text, dtype: object, 8:                  id                             ...      0        0       0       0              0  , 10:                  id                             ...      0        0       0       0              0  , 11:                  id                             ...      0        0       0       0              0  , 13: id               object\ncomment_text     object\n...      int64\nidentity_hate     int64\ndtype: object, 15:                  id                             ...      0        0       0       0              0  , 17:                  id                             ...      0        0       0       0              0  }, 'TfidfVectorizer': <class 'sklearn.feature_extraction.text.TfidfVectorizer'>, '_':                  id                             ...      0        0       0       0              0  , '_10':                  id                             ...      0        0       0       0              0  , '_11':                  id                             ...      0        0       0       0              0  , '_13': id               object\ncomment_text     object\n...      int64\nidentity_hate     int64\ndtype: object, '_15':                  id                             ...      0        0       0       0              0  , '_17':                  id                             ...      0        0       0       0              0  , ...}\n   2863             finally:\n   2864                 # Reset our crash handler in place\n   2865                 sys.excepthook = old_excepthook\n   2866         except SystemExit as e:\n\n...........................................................................\n/home/dan/Test/example/<ipython-input-20-2f1297c59012> in <module>()\n----> 1 vectorizer.fit(all_text)\n      2 train_features = vectorizer.transform(train_text)\n      3 test_features = vectorizer.transform(test_text)\n\n...........................................................................\n/home/dan/anaconda3/lib/python3.6/site-packages/sklearn/pipeline.py in fit(self=FeatureUnion(n_jobs=2,\n       transformer_list=[...abulary=None))],\n       transformer_weights=None), X=0         Explanation\\nWhy the edits made under ...Name: comment_text, Length: 312735, dtype: object, y=None)\n    709         \"\"\"\n    710         self.transformer_list = list(self.transformer_list)\n    711         self._validate_transformers()\n    712         transformers = Parallel(n_jobs=self.n_jobs)(\n    713             delayed(_fit_one_transformer)(trans, X, y)\n--> 714             for _, trans, _ in self._iter())\n        self._iter = <bound method FeatureUnion._iter of FeatureUnion...bulary=None))],\n       transformer_weights=None)>\n    715         self._update_transformer_list(transformers)\n    716         return self\n    717 \n    718     def fit_transform(self, X, y=None, **fit_params):\n\n...........................................................................\n/home/dan/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=Parallel(n_jobs=2), iterable=<generator object FeatureUnion.fit.<locals>.<genexpr>>)\n    784             if pre_dispatch == \"all\" or n_jobs == 1:\n    785                 # The iterable was consumed all at once by the above for loop.\n    786                 # No need to wait for async callbacks to trigger to\n    787                 # consumption.\n    788                 self._iterating = False\n--> 789             self.retrieve()\n        self.retrieve = <bound method Parallel.retrieve of Parallel(n_jobs=2)>\n    790             # Make sure that we get a last message telling us we are done\n    791             elapsed_time = time.time() - self._start_time\n    792             self._print('Done %3i out of %3i | elapsed: %s finished',\n    793                         (len(self._output), len(self._output),\n\n---------------------------------------------------------------------------\nSub-process traceback:\n---------------------------------------------------------------------------\nMemoryError                                        Wed Aug  8 13:35:29 2018\nPID: 24953                     Python 3.6.3: /home/dan/anaconda3/bin/python\n...........................................................................\n/home/dan/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        self.items = [(<function _fit_one_transformer>, (TfidfVectorizer(analyzer='char', binary=False, d...izer=None, use_idf=True,\n        vocabulary=None), 0         Explanation\\nWhy the edits made under ...Name: comment_text, Length: 312735, dtype: object, None), {})]\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\n/home/dan/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in <listcomp>(.0=<list_iterator object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        func = <function _fit_one_transformer>\n        args = (TfidfVectorizer(analyzer='char', binary=False, d...izer=None, use_idf=True,\n        vocabulary=None), 0         Explanation\\nWhy the edits made under ...Name: comment_text, Length: 312735, dtype: object, None)\n        kwargs = {}\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\n/home/dan/anaconda3/lib/python3.6/site-packages/sklearn/pipeline.py in _fit_one_transformer(transformer=TfidfVectorizer(analyzer='char', binary=False, d...izer=None, use_idf=True,\n        vocabulary=None), X=0         Explanation\\nWhy the edits made under ...Name: comment_text, Length: 312735, dtype: object, y=None)\n    562                         .format(list(kwargs.keys())[0]))\n    563     return Pipeline(_name_estimators(steps), memory=memory)\n    564 \n    565 \n    566 def _fit_one_transformer(transformer, X, y):\n--> 567     return transformer.fit(X, y)\n        transformer.fit = <bound method TfidfVectorizer.fit of TfidfVector...zer=None, use_idf=True,\n        vocabulary=None)>\n        X = 0         Explanation\\nWhy the edits made under ...Name: comment_text, Length: 312735, dtype: object\n        y = None\n    568 \n    569 \n    570 def _transform_one(transformer, weight, X):\n    571     res = transformer.transform(X)\n\n...........................................................................\n/home/dan/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py in fit(self=TfidfVectorizer(analyzer='char', binary=False, d...izer=None, use_idf=True,\n        vocabulary=None), raw_documents=0         Explanation\\nWhy the edits made under ...Name: comment_text, Length: 312735, dtype: object, y=None)\n   1356 \n   1357         Returns\n   1358         -------\n   1359         self : TfidfVectorizer\n   1360         \"\"\"\n-> 1361         X = super(TfidfVectorizer, self).fit_transform(raw_documents)\n        X = undefined\n        self.fit_transform = <bound method TfidfVectorizer.fit_transform of T...zer=None, use_idf=True,\n        vocabulary=None)>\n        raw_documents = 0         Explanation\\nWhy the edits made under ...Name: comment_text, Length: 312735, dtype: object\n   1362         self._tfidf.fit(X)\n   1363         return self\n   1364 \n   1365     def fit_transform(self, raw_documents, y=None):\n\n...........................................................................\n/home/dan/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py in fit_transform(self=TfidfVectorizer(analyzer='char', binary=False, d...izer=None, use_idf=True,\n        vocabulary=None), raw_documents=0         Explanation\\nWhy the edits made under ...Name: comment_text, Length: 312735, dtype: object, y=None)\n    870 \n    871         if self.binary:\n    872             X.data.fill(1)\n    873 \n    874         if not self.fixed_vocabulary_:\n--> 875             X = self._sort_features(X, vocabulary)\n        X = <312735x1181355 sparse matrix of type '<class 'n... stored elements in Compressed Sparse Row format>\n        self._sort_features = <bound method CountVectorizer._sort_features of ...zer=None, use_idf=True,\n        vocabulary=None)>\n        vocabulary = {'\\x02': 0, '\\x02 ': 1, '\\x02 n': 2, '\\x02 ni': 3, '\\n': 4, '\\n!': 5, '\\n! ': 6, '\\n! !': 7, '\\n! a': 8, '\\n! b': 9, ...}\n    876 \n    877             n_doc = X.shape[0]\n    878             max_doc_count = (max_df\n    879                              if isinstance(max_df, numbers.Integral)\n\n...........................................................................\n/home/dan/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py in _sort_features(self=TfidfVectorizer(analyzer='char', binary=False, d...izer=None, use_idf=True,\n        vocabulary=None), X=<312735x1181355 sparse matrix of type '<class 'n... stored elements in Compressed Sparse Row format>, vocabulary={'\\x02': 0, '\\x02 ': 1, '\\x02 n': 2, '\\x02 ni': 3, '\\n': 4, '\\n!': 5, '\\n! ': 6, '\\n! !': 7, '\\n! a': 8, '\\n! b': 9, ...})\n    726         map_index = np.empty(len(sorted_features), dtype=np.int32)\n    727         for new_val, (term, old_val) in enumerate(sorted_features):\n    728             vocabulary[term] = new_val\n    729             map_index[old_val] = new_val\n    730 \n--> 731         X.indices = map_index.take(X.indices, mode='clip')\n        X.indices = array([     0,      1,      2, ..., 220672, 393629, 437212], dtype=int32)\n        map_index.take = <built-in method take of numpy.ndarray object>\n    732         return X\n    733 \n    734     def _limit_features(self, X, vocabulary, high=None, low=None,\n    735                         limit=None):\n\nMemoryError: \n___________________________________________________________________________",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/home/dan/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\", line 350, in __call__\n    return self.func(*args, **kwargs)\n  File \"/home/dan/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\", line 131, in __call__\n    return [func(*args, **kwargs) for func, args, kwargs in self.items]\n  File \"/home/dan/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\", line 131, in <listcomp>\n    return [func(*args, **kwargs) for func, args, kwargs in self.items]\n  File \"/home/dan/anaconda3/lib/python3.6/site-packages/sklearn/pipeline.py\", line 567, in _fit_one_transformer\n    return transformer.fit(X, y)\n  File \"/home/dan/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 1361, in fit\n    X = super(TfidfVectorizer, self).fit_transform(raw_documents)\n  File \"/home/dan/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 875, in fit_transform\n    X = self._sort_features(X, vocabulary)\n  File \"/home/dan/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 731, in _sort_features\n    X.indices = map_index.take(X.indices, mode='clip')\nMemoryError\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/dan/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n    result = (True, func(*args, **kwds))\n  File \"/home/dan/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\", line 359, in __call__\n    raise TransportableException(text, e_type)\nsklearn.externals.joblib.my_exceptions.TransportableException: TransportableException\n___________________________________________________________________________\nMemoryError                                        Wed Aug  8 13:35:29 2018\nPID: 24953                     Python 3.6.3: /home/dan/anaconda3/bin/python\n...........................................................................\n/home/dan/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        self.items = [(<function _fit_one_transformer>, (TfidfVectorizer(analyzer='char', binary=False, d...izer=None, use_idf=True,\n        vocabulary=None), 0         Explanation\\nWhy the edits made under ...Name: comment_text, Length: 312735, dtype: object, None), {})]\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\n/home/dan/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in <listcomp>(.0=<list_iterator object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        func = <function _fit_one_transformer>\n        args = (TfidfVectorizer(analyzer='char', binary=False, d...izer=None, use_idf=True,\n        vocabulary=None), 0         Explanation\\nWhy the edits made under ...Name: comment_text, Length: 312735, dtype: object, None)\n        kwargs = {}\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\n/home/dan/anaconda3/lib/python3.6/site-packages/sklearn/pipeline.py in _fit_one_transformer(transformer=TfidfVectorizer(analyzer='char', binary=False, d...izer=None, use_idf=True,\n        vocabulary=None), X=0         Explanation\\nWhy the edits made under ...Name: comment_text, Length: 312735, dtype: object, y=None)\n    562                         .format(list(kwargs.keys())[0]))\n    563     return Pipeline(_name_estimators(steps), memory=memory)\n    564 \n    565 \n    566 def _fit_one_transformer(transformer, X, y):\n--> 567     return transformer.fit(X, y)\n        transformer.fit = <bound method TfidfVectorizer.fit of TfidfVector...zer=None, use_idf=True,\n        vocabulary=None)>\n        X = 0         Explanation\\nWhy the edits made under ...Name: comment_text, Length: 312735, dtype: object\n        y = None\n    568 \n    569 \n    570 def _transform_one(transformer, weight, X):\n    571     res = transformer.transform(X)\n\n...........................................................................\n/home/dan/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py in fit(self=TfidfVectorizer(analyzer='char', binary=False, d...izer=None, use_idf=True,\n        vocabulary=None), raw_documents=0         Explanation\\nWhy the edits made under ...Name: comment_text, Length: 312735, dtype: object, y=None)\n   1356 \n   1357         Returns\n   1358         -------\n   1359         self : TfidfVectorizer\n   1360         \"\"\"\n-> 1361         X = super(TfidfVectorizer, self).fit_transform(raw_documents)\n        X = undefined\n        self.fit_transform = <bound method TfidfVectorizer.fit_transform of T...zer=None, use_idf=True,\n        vocabulary=None)>\n        raw_documents = 0         Explanation\\nWhy the edits made under ...Name: comment_text, Length: 312735, dtype: object\n   1362         self._tfidf.fit(X)\n   1363         return self\n   1364 \n   1365     def fit_transform(self, raw_documents, y=None):\n\n...........................................................................\n/home/dan/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py in fit_transform(self=TfidfVectorizer(analyzer='char', binary=False, d...izer=None, use_idf=True,\n        vocabulary=None), raw_documents=0         Explanation\\nWhy the edits made under ...Name: comment_text, Length: 312735, dtype: object, y=None)\n    870 \n    871         if self.binary:\n    872             X.data.fill(1)\n    873 \n    874         if not self.fixed_vocabulary_:\n--> 875             X = self._sort_features(X, vocabulary)\n        X = <312735x1181355 sparse matrix of type '<class 'n... stored elements in Compressed Sparse Row format>\n        self._sort_features = <bound method CountVectorizer._sort_features of ...zer=None, use_idf=True,\n        vocabulary=None)>\n        vocabulary = {'\\x02': 0, '\\x02 ': 1, '\\x02 n': 2, '\\x02 ni': 3, '\\n': 4, '\\n!': 5, '\\n! ': 6, '\\n! !': 7, '\\n! a': 8, '\\n! b': 9, ...}\n    876 \n    877             n_doc = X.shape[0]\n    878             max_doc_count = (max_df\n    879                              if isinstance(max_df, numbers.Integral)\n\n...........................................................................\n/home/dan/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py in _sort_features(self=TfidfVectorizer(analyzer='char', binary=False, d...izer=None, use_idf=True,\n        vocabulary=None), X=<312735x1181355 sparse matrix of type '<class 'n... stored elements in Compressed Sparse Row format>, vocabulary={'\\x02': 0, '\\x02 ': 1, '\\x02 n': 2, '\\x02 ni': 3, '\\n': 4, '\\n!': 5, '\\n! ': 6, '\\n! !': 7, '\\n! a': 8, '\\n! b': 9, ...})\n    726         map_index = np.empty(len(sorted_features), dtype=np.int32)\n    727         for new_val, (term, old_val) in enumerate(sorted_features):\n    728             vocabulary[term] = new_val\n    729             map_index[old_val] = new_val\n    730 \n--> 731         X.indices = map_index.take(X.indices, mode='clip')\n        X.indices = array([     0,      1,      2, ..., 220672, 393629, 437212], dtype=int32)\n        map_index.take = <built-in method take of numpy.ndarray object>\n    732         return X\n    733 \n    734     def _limit_features(self, X, vocabulary, high=None, low=None,\n    735                         limit=None):\n\nMemoryError: \n___________________________________________________________________________\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mTransportableException\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    700\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTransportableException\u001b[0m: TransportableException\n___________________________________________________________________________\nMemoryError                                        Wed Aug  8 13:35:29 2018\nPID: 24953                     Python 3.6.3: /home/dan/anaconda3/bin/python\n...........................................................................\n/home/dan/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        self.items = [(<function _fit_one_transformer>, (TfidfVectorizer(analyzer='char', binary=False, d...izer=None, use_idf=True,\n        vocabulary=None), 0         Explanation\\nWhy the edits made under ...Name: comment_text, Length: 312735, dtype: object, None), {})]\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\n/home/dan/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in <listcomp>(.0=<list_iterator object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        func = <function _fit_one_transformer>\n        args = (TfidfVectorizer(analyzer='char', binary=False, d...izer=None, use_idf=True,\n        vocabulary=None), 0         Explanation\\nWhy the edits made under ...Name: comment_text, Length: 312735, dtype: object, None)\n        kwargs = {}\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\n/home/dan/anaconda3/lib/python3.6/site-packages/sklearn/pipeline.py in _fit_one_transformer(transformer=TfidfVectorizer(analyzer='char', binary=False, d...izer=None, use_idf=True,\n        vocabulary=None), X=0         Explanation\\nWhy the edits made under ...Name: comment_text, Length: 312735, dtype: object, y=None)\n    562                         .format(list(kwargs.keys())[0]))\n    563     return Pipeline(_name_estimators(steps), memory=memory)\n    564 \n    565 \n    566 def _fit_one_transformer(transformer, X, y):\n--> 567     return transformer.fit(X, y)\n        transformer.fit = <bound method TfidfVectorizer.fit of TfidfVector...zer=None, use_idf=True,\n        vocabulary=None)>\n        X = 0         Explanation\\nWhy the edits made under ...Name: comment_text, Length: 312735, dtype: object\n        y = None\n    568 \n    569 \n    570 def _transform_one(transformer, weight, X):\n    571     res = transformer.transform(X)\n\n...........................................................................\n/home/dan/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py in fit(self=TfidfVectorizer(analyzer='char', binary=False, d...izer=None, use_idf=True,\n        vocabulary=None), raw_documents=0         Explanation\\nWhy the edits made under ...Name: comment_text, Length: 312735, dtype: object, y=None)\n   1356 \n   1357         Returns\n   1358         -------\n   1359         self : TfidfVectorizer\n   1360         \"\"\"\n-> 1361         X = super(TfidfVectorizer, self).fit_transform(raw_documents)\n        X = undefined\n        self.fit_transform = <bound method TfidfVectorizer.fit_transform of T...zer=None, use_idf=True,\n        vocabulary=None)>\n        raw_documents = 0         Explanation\\nWhy the edits made under ...Name: comment_text, Length: 312735, dtype: object\n   1362         self._tfidf.fit(X)\n   1363         return self\n   1364 \n   1365     def fit_transform(self, raw_documents, y=None):\n\n...........................................................................\n/home/dan/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py in fit_transform(self=TfidfVectorizer(analyzer='char', binary=False, d...izer=None, use_idf=True,\n        vocabulary=None), raw_documents=0         Explanation\\nWhy the edits made under ...Name: comment_text, Length: 312735, dtype: object, y=None)\n    870 \n    871         if self.binary:\n    872             X.data.fill(1)\n    873 \n    874         if not self.fixed_vocabulary_:\n--> 875             X = self._sort_features(X, vocabulary)\n        X = <312735x1181355 sparse matrix of type '<class 'n... stored elements in Compressed Sparse Row format>\n        self._sort_features = <bound method CountVectorizer._sort_features of ...zer=None, use_idf=True,\n        vocabulary=None)>\n        vocabulary = {'\\x02': 0, '\\x02 ': 1, '\\x02 n': 2, '\\x02 ni': 3, '\\n': 4, '\\n!': 5, '\\n! ': 6, '\\n! !': 7, '\\n! a': 8, '\\n! b': 9, ...}\n    876 \n    877             n_doc = X.shape[0]\n    878             max_doc_count = (max_df\n    879                              if isinstance(max_df, numbers.Integral)\n\n...........................................................................\n/home/dan/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py in _sort_features(self=TfidfVectorizer(analyzer='char', binary=False, d...izer=None, use_idf=True,\n        vocabulary=None), X=<312735x1181355 sparse matrix of type '<class 'n... stored elements in Compressed Sparse Row format>, vocabulary={'\\x02': 0, '\\x02 ': 1, '\\x02 n': 2, '\\x02 ni': 3, '\\n': 4, '\\n!': 5, '\\n! ': 6, '\\n! !': 7, '\\n! a': 8, '\\n! b': 9, ...})\n    726         map_index = np.empty(len(sorted_features), dtype=np.int32)\n    727         for new_val, (term, old_val) in enumerate(sorted_features):\n    728             vocabulary[term] = new_val\n    729             map_index[old_val] = new_val\n    730 \n--> 731         X.indices = map_index.take(X.indices, mode='clip')\n        X.indices = array([     0,      1,      2, ..., 220672, 393629, 437212], dtype=int32)\n        map_index.take = <built-in method take of numpy.ndarray object>\n    732         return X\n    733 \n    734     def _limit_features(self, X, vocabulary, high=None, low=None,\n    735                         limit=None):\n\nMemoryError: \n___________________________________________________________________________",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mJoblibMemoryError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-2f1297c59012>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtrain_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtest_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    712\u001b[0m         transformers = Parallel(n_jobs=self.n_jobs)(\n\u001b[1;32m    713\u001b[0m             \u001b[0mdelayed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_fit_one_transformer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 714\u001b[0;31m             for _, trans, _ in self._iter())\n\u001b[0m\u001b[1;32m    715\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_transformer_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    787\u001b[0m                 \u001b[0;31m# consumption.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    790\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    738\u001b[0m                     \u001b[0mexception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 740\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mJoblibMemoryError\u001b[0m: JoblibMemoryError\n___________________________________________________________________________\nMultiprocessing exception:\n...........................................................................\n/home/dan/anaconda3/lib/python3.6/runpy.py in _run_module_as_main(mod_name='ipykernel_launcher', alter_argv=1)\n    188         sys.exit(msg)\n    189     main_globals = sys.modules[\"__main__\"].__dict__\n    190     if alter_argv:\n    191         sys.argv[0] = mod_spec.origin\n    192     return _run_code(code, main_globals, None,\n--> 193                      \"__main__\", mod_spec)\n        mod_spec = ModuleSpec(name='ipykernel_launcher', loader=<_f...b/python3.6/site-packages/ipykernel_launcher.py')\n    194 \n    195 def run_module(mod_name, init_globals=None,\n    196                run_name=None, alter_sys=False):\n    197     \"\"\"Execute a module's code without importing it\n\n...........................................................................\n/home/dan/anaconda3/lib/python3.6/runpy.py in _run_code(code=<code object <module> at 0x7f51e67915d0, file \"/...3.6/site-packages/ipykernel_launcher.py\", line 5>, run_globals={'__annotations__': {}, '__builtins__': <module 'builtins' (built-in)>, '__cached__': '/home/dan/anaconda3/lib/python3.6/site-packages/__pycache__/ipykernel_launcher.cpython-36.pyc', '__doc__': 'Entry point for launching an IPython kernel.\\n\\nTh...orts until\\nafter removing the cwd from sys.path.\\n', '__file__': '/home/dan/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py', '__loader__': <_frozen_importlib_external.SourceFileLoader object>, '__name__': '__main__', '__package__': '', '__spec__': ModuleSpec(name='ipykernel_launcher', loader=<_f...b/python3.6/site-packages/ipykernel_launcher.py'), 'app': <module 'ipykernel.kernelapp' from '/home/dan/an.../python3.6/site-packages/ipykernel/kernelapp.py'>, ...}, init_globals=None, mod_name='__main__', mod_spec=ModuleSpec(name='ipykernel_launcher', loader=<_f...b/python3.6/site-packages/ipykernel_launcher.py'), pkg_name='', script_name=None)\n     80                        __cached__ = cached,\n     81                        __doc__ = None,\n     82                        __loader__ = loader,\n     83                        __package__ = pkg_name,\n     84                        __spec__ = mod_spec)\n---> 85     exec(code, run_globals)\n        code = <code object <module> at 0x7f51e67915d0, file \"/...3.6/site-packages/ipykernel_launcher.py\", line 5>\n        run_globals = {'__annotations__': {}, '__builtins__': <module 'builtins' (built-in)>, '__cached__': '/home/dan/anaconda3/lib/python3.6/site-packages/__pycache__/ipykernel_launcher.cpython-36.pyc', '__doc__': 'Entry point for launching an IPython kernel.\\n\\nTh...orts until\\nafter removing the cwd from sys.path.\\n', '__file__': '/home/dan/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py', '__loader__': <_frozen_importlib_external.SourceFileLoader object>, '__name__': '__main__', '__package__': '', '__spec__': ModuleSpec(name='ipykernel_launcher', loader=<_f...b/python3.6/site-packages/ipykernel_launcher.py'), 'app': <module 'ipykernel.kernelapp' from '/home/dan/an.../python3.6/site-packages/ipykernel/kernelapp.py'>, ...}\n     86     return run_globals\n     87 \n     88 def _run_module_code(code, init_globals=None,\n     89                     mod_name=None, mod_spec=None,\n\n...........................................................................\n/home/dan/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py in <module>()\n     11     # This is added back by InteractiveShellApp.init_path()\n     12     if sys.path[0] == '':\n     13         del sys.path[0]\n     14 \n     15     from ipykernel import kernelapp as app\n---> 16     app.launch_new_instance()\n\n...........................................................................\n/home/dan/anaconda3/lib/python3.6/site-packages/traitlets/config/application.py in launch_instance(cls=<class 'ipykernel.kernelapp.IPKernelApp'>, argv=None, **kwargs={})\n    653 \n    654         If a global instance already exists, this reinitializes and starts it\n    655         \"\"\"\n    656         app = cls.instance(**kwargs)\n    657         app.initialize(argv)\n--> 658         app.start()\n        app.start = <bound method IPKernelApp.start of <ipykernel.kernelapp.IPKernelApp object>>\n    659 \n    660 #-----------------------------------------------------------------------------\n    661 # utility functions, for convenience\n    662 #-----------------------------------------------------------------------------\n\n...........................................................................\n/home/dan/anaconda3/lib/python3.6/site-packages/ipykernel/kernelapp.py in start(self=<ipykernel.kernelapp.IPKernelApp object>)\n    472             return self.subapp.start()\n    473         if self.poller is not None:\n    474             self.poller.start()\n    475         self.kernel.start()\n    476         try:\n--> 477             ioloop.IOLoop.instance().start()\n    478         except KeyboardInterrupt:\n    479             pass\n    480 \n    481 launch_new_instance = IPKernelApp.launch_instance\n\n...........................................................................\n/home/dan/anaconda3/lib/python3.6/site-packages/zmq/eventloop/ioloop.py in start(self=<zmq.eventloop.ioloop.ZMQIOLoop object>)\n    172             )\n    173         return loop\n    174     \n    175     def start(self):\n    176         try:\n--> 177             super(ZMQIOLoop, self).start()\n        self.start = <bound method ZMQIOLoop.start of <zmq.eventloop.ioloop.ZMQIOLoop object>>\n    178         except ZMQError as e:\n    179             if e.errno == ETERM:\n    180                 # quietly return on ETERM\n    181                 pass\n\n...........................................................................\n/home/dan/anaconda3/lib/python3.6/site-packages/tornado/ioloop.py in start(self=<zmq.eventloop.ioloop.ZMQIOLoop object>)\n    883                 self._events.update(event_pairs)\n    884                 while self._events:\n    885                     fd, events = self._events.popitem()\n    886                     try:\n    887                         fd_obj, handler_func = self._handlers[fd]\n--> 888                         handler_func(fd_obj, events)\n        handler_func = <function wrap.<locals>.null_wrapper>\n        fd_obj = <zmq.sugar.socket.Socket object>\n        events = 1\n    889                     except (OSError, IOError) as e:\n    890                         if errno_from_exception(e) == errno.EPIPE:\n    891                             # Happens when the client closes the connection\n    892                             pass\n\n...........................................................................\n/home/dan/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py in null_wrapper(*args=(<zmq.sugar.socket.Socket object>, 1), **kwargs={})\n    272         # Fast path when there are no active contexts.\n    273         def null_wrapper(*args, **kwargs):\n    274             try:\n    275                 current_state = _state.contexts\n    276                 _state.contexts = cap_contexts[0]\n--> 277                 return fn(*args, **kwargs)\n        args = (<zmq.sugar.socket.Socket object>, 1)\n        kwargs = {}\n    278             finally:\n    279                 _state.contexts = current_state\n    280         null_wrapper._wrapped = True\n    281         return null_wrapper\n\n...........................................................................\n/home/dan/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py in _handle_events(self=<zmq.eventloop.zmqstream.ZMQStream object>, fd=<zmq.sugar.socket.Socket object>, events=1)\n    435             # dispatch events:\n    436             if events & IOLoop.ERROR:\n    437                 gen_log.error(\"got POLLERR event on ZMQStream, which doesn't make sense\")\n    438                 return\n    439             if events & IOLoop.READ:\n--> 440                 self._handle_recv()\n        self._handle_recv = <bound method ZMQStream._handle_recv of <zmq.eventloop.zmqstream.ZMQStream object>>\n    441                 if not self.socket:\n    442                     return\n    443             if events & IOLoop.WRITE:\n    444                 self._handle_send()\n\n...........................................................................\n/home/dan/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py in _handle_recv(self=<zmq.eventloop.zmqstream.ZMQStream object>)\n    467                 gen_log.error(\"RECV Error: %s\"%zmq.strerror(e.errno))\n    468         else:\n    469             if self._recv_callback:\n    470                 callback = self._recv_callback\n    471                 # self._recv_callback = None\n--> 472                 self._run_callback(callback, msg)\n        self._run_callback = <bound method ZMQStream._run_callback of <zmq.eventloop.zmqstream.ZMQStream object>>\n        callback = <function wrap.<locals>.null_wrapper>\n        msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]\n    473                 \n    474         # self.update_state()\n    475         \n    476 \n\n...........................................................................\n/home/dan/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py in _run_callback(self=<zmq.eventloop.zmqstream.ZMQStream object>, callback=<function wrap.<locals>.null_wrapper>, *args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})\n    409         close our socket.\"\"\"\n    410         try:\n    411             # Use a NullContext to ensure that all StackContexts are run\n    412             # inside our blanket exception handler rather than outside.\n    413             with stack_context.NullContext():\n--> 414                 callback(*args, **kwargs)\n        callback = <function wrap.<locals>.null_wrapper>\n        args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)\n        kwargs = {}\n    415         except:\n    416             gen_log.error(\"Uncaught exception, closing connection.\",\n    417                           exc_info=True)\n    418             # Close the socket on an uncaught exception from a user callback\n\n...........................................................................\n/home/dan/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py in null_wrapper(*args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})\n    272         # Fast path when there are no active contexts.\n    273         def null_wrapper(*args, **kwargs):\n    274             try:\n    275                 current_state = _state.contexts\n    276                 _state.contexts = cap_contexts[0]\n--> 277                 return fn(*args, **kwargs)\n        args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)\n        kwargs = {}\n    278             finally:\n    279                 _state.contexts = current_state\n    280         null_wrapper._wrapped = True\n    281         return null_wrapper\n\n...........................................................................\n/home/dan/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py in dispatcher(msg=[<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>])\n    278         if self.control_stream:\n    279             self.control_stream.on_recv(self.dispatch_control, copy=False)\n    280 \n    281         def make_dispatcher(stream):\n    282             def dispatcher(msg):\n--> 283                 return self.dispatch_shell(stream, msg)\n        msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]\n    284             return dispatcher\n    285 \n    286         for s in self.shell_streams:\n    287             s.on_recv(make_dispatcher(s), copy=False)\n\n...........................................................................\n/home/dan/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py in dispatch_shell(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, msg={'buffers': [], 'content': {'allow_stdin': True, 'code': 'vectorizer.fit(all_text)\\ntrain_features = vector...)\\ntest_features = vectorizer.transform(test_text)', 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2018, 8, 8, 10, 29, 39, 451027, tzinfo=tzutc()), 'msg_id': '0A57D0D6673042B2AA1F61ADFF6B2C96', 'msg_type': 'execute_request', 'session': '17FB4326E53D41888A6060F835E8A03C', 'username': 'username', 'version': '5.0'}, 'metadata': {}, 'msg_id': '0A57D0D6673042B2AA1F61ADFF6B2C96', 'msg_type': 'execute_request', 'parent_header': {}})\n    230             self.log.warn(\"Unknown message type: %r\", msg_type)\n    231         else:\n    232             self.log.debug(\"%s: %s\", msg_type, msg)\n    233             self.pre_handler_hook()\n    234             try:\n--> 235                 handler(stream, idents, msg)\n        handler = <bound method Kernel.execute_request of <ipykernel.ipkernel.IPythonKernel object>>\n        stream = <zmq.eventloop.zmqstream.ZMQStream object>\n        idents = [b'17FB4326E53D41888A6060F835E8A03C']\n        msg = {'buffers': [], 'content': {'allow_stdin': True, 'code': 'vectorizer.fit(all_text)\\ntrain_features = vector...)\\ntest_features = vectorizer.transform(test_text)', 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2018, 8, 8, 10, 29, 39, 451027, tzinfo=tzutc()), 'msg_id': '0A57D0D6673042B2AA1F61ADFF6B2C96', 'msg_type': 'execute_request', 'session': '17FB4326E53D41888A6060F835E8A03C', 'username': 'username', 'version': '5.0'}, 'metadata': {}, 'msg_id': '0A57D0D6673042B2AA1F61ADFF6B2C96', 'msg_type': 'execute_request', 'parent_header': {}}\n    236             except Exception:\n    237                 self.log.error(\"Exception in message handler:\", exc_info=True)\n    238             finally:\n    239                 self.post_handler_hook()\n\n...........................................................................\n/home/dan/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py in execute_request(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, ident=[b'17FB4326E53D41888A6060F835E8A03C'], parent={'buffers': [], 'content': {'allow_stdin': True, 'code': 'vectorizer.fit(all_text)\\ntrain_features = vector...)\\ntest_features = vectorizer.transform(test_text)', 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2018, 8, 8, 10, 29, 39, 451027, tzinfo=tzutc()), 'msg_id': '0A57D0D6673042B2AA1F61ADFF6B2C96', 'msg_type': 'execute_request', 'session': '17FB4326E53D41888A6060F835E8A03C', 'username': 'username', 'version': '5.0'}, 'metadata': {}, 'msg_id': '0A57D0D6673042B2AA1F61ADFF6B2C96', 'msg_type': 'execute_request', 'parent_header': {}})\n    394         if not silent:\n    395             self.execution_count += 1\n    396             self._publish_execute_input(code, parent, self.execution_count)\n    397 \n    398         reply_content = self.do_execute(code, silent, store_history,\n--> 399                                         user_expressions, allow_stdin)\n        user_expressions = {}\n        allow_stdin = True\n    400 \n    401         # Flush output before sending the reply.\n    402         sys.stdout.flush()\n    403         sys.stderr.flush()\n\n...........................................................................\n/home/dan/anaconda3/lib/python3.6/site-packages/ipykernel/ipkernel.py in do_execute(self=<ipykernel.ipkernel.IPythonKernel object>, code='vectorizer.fit(all_text)\\ntrain_features = vector...)\\ntest_features = vectorizer.transform(test_text)', silent=False, store_history=True, user_expressions={}, allow_stdin=True)\n    191 \n    192         self._forward_input(allow_stdin)\n    193 \n    194         reply_content = {}\n    195         try:\n--> 196             res = shell.run_cell(code, store_history=store_history, silent=silent)\n        res = undefined\n        shell.run_cell = <bound method ZMQInteractiveShell.run_cell of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        code = 'vectorizer.fit(all_text)\\ntrain_features = vector...)\\ntest_features = vectorizer.transform(test_text)'\n        store_history = True\n        silent = False\n    197         finally:\n    198             self._restore_input()\n    199 \n    200         if res.error_before_exec is not None:\n\n...........................................................................\n/home/dan/anaconda3/lib/python3.6/site-packages/ipykernel/zmqshell.py in run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, *args=('vectorizer.fit(all_text)\\ntrain_features = vector...)\\ntest_features = vectorizer.transform(test_text)',), **kwargs={'silent': False, 'store_history': True})\n    528             )\n    529         self.payload_manager.write_payload(payload)\n    530 \n    531     def run_cell(self, *args, **kwargs):\n    532         self._last_traceback = None\n--> 533         return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n        self.run_cell = <bound method ZMQInteractiveShell.run_cell of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        args = ('vectorizer.fit(all_text)\\ntrain_features = vector...)\\ntest_features = vectorizer.transform(test_text)',)\n        kwargs = {'silent': False, 'store_history': True}\n    534 \n    535     def _showtraceback(self, etype, evalue, stb):\n    536         # try to preserve ordering of tracebacks and print statements\n    537         sys.stdout.flush()\n\n...........................................................................\n/home/dan/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py in run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, raw_cell='vectorizer.fit(all_text)\\ntrain_features = vector...)\\ntest_features = vectorizer.transform(test_text)', store_history=True, silent=False, shell_futures=True)\n   2693                 self.displayhook.exec_result = result\n   2694 \n   2695                 # Execute the user code\n   2696                 interactivity = \"none\" if silent else self.ast_node_interactivity\n   2697                 has_raised = self.run_ast_nodes(code_ast.body, cell_name,\n-> 2698                    interactivity=interactivity, compiler=compiler, result=result)\n        interactivity = 'last_expr'\n        compiler = <IPython.core.compilerop.CachingCompiler object>\n   2699                 \n   2700                 self.last_execution_succeeded = not has_raised\n   2701 \n   2702                 # Reset this so later displayed values do not modify the\n\n...........................................................................\n/home/dan/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py in run_ast_nodes(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, nodelist=[<_ast.Expr object>, <_ast.Assign object>, <_ast.Assign object>], cell_name='<ipython-input-20-2f1297c59012>', interactivity='none', compiler=<IPython.core.compilerop.CachingCompiler object>, result=<ExecutionResult object at 7f51ac53eb38, executi..._before_exec=None error_in_exec=None result=None>)\n   2797 \n   2798         try:\n   2799             for i, node in enumerate(to_run_exec):\n   2800                 mod = ast.Module([node])\n   2801                 code = compiler(mod, cell_name, \"exec\")\n-> 2802                 if self.run_code(code, result):\n        self.run_code = <bound method InteractiveShell.run_code of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        code = <code object <module> at 0x7f51a0033c90, file \"<ipython-input-20-2f1297c59012>\", line 1>\n        result = <ExecutionResult object at 7f51ac53eb38, executi..._before_exec=None error_in_exec=None result=None>\n   2803                     return True\n   2804 \n   2805             for i, node in enumerate(to_run_interactive):\n   2806                 mod = ast.Interactive([node])\n\n...........................................................................\n/home/dan/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py in run_code(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, code_obj=<code object <module> at 0x7f51a0033c90, file \"<ipython-input-20-2f1297c59012>\", line 1>, result=<ExecutionResult object at 7f51ac53eb38, executi..._before_exec=None error_in_exec=None result=None>)\n   2857         outflag = True  # happens in more places, so it's easier as default\n   2858         try:\n   2859             try:\n   2860                 self.hooks.pre_run_code_hook()\n   2861                 #rprint('Running code', repr(code_obj)) # dbg\n-> 2862                 exec(code_obj, self.user_global_ns, self.user_ns)\n        code_obj = <code object <module> at 0x7f51a0033c90, file \"<ipython-input-20-2f1297c59012>\", line 1>\n        self.user_global_ns = {'In': ['', 'import numpy as np\\nimport pandas as pd\\nfrom skle...val_score\\nfrom sklearn.pipeline import make_union', \"train = pd.read_csv('/home/dan/Test/Kaggle/data/...an/Test/Kaggle/data/TF_idf/test.csv').fillna(' ')\", 'train[0]', 'train.head()', \"train = pd.read_csv('/home/dan/Test/Kaggle/data/...an/Test/Kaggle/data/TF_idf/test.csv').fillna(' ')\", 'train.head()', \"train['comment_text'].head()\", 'train.head()', \"train = pd.read_csv('/home/dan/Test/Kaggle/data/...an/Test/Kaggle/data/TF_idf/test.csv').fillna(' ')\", 'train.head()', 'train.head()', \"train = pd.read_csv('/home/dan/Test/Kaggle/data/...an/Test/Kaggle/data/TF_idf/test.csv').fillna(' ')\", 'train.dtypes', 'train.info()', 'train.head()', \"train = pd.read_csv('/home/dan/Test/Kaggle/data/...an/Test/Kaggle/data/TF_idf/test.csv').fillna(' ')\", 'train.head()', \"class_names = ['toxic', 'severe_toxic', 'obscene...t']\\nall_text = pd.concat([train_text, test_text])\", 'word_vectorizer = TfidfVectorizer(\\n    sublinear...union(word_vectorizer, char_vectorizer, n_jobs=2)', ...], 'LogisticRegression': <class 'sklearn.linear_model.logistic.LogisticRegression'>, 'Out': {4:                  id                             ...      0        0       0       0              0  , 6:                  id                             ...      0        0       0       0              0  , 7: 0    Explanation\\nWhy the edits made under my us...you remember...\nName: comment_text, dtype: object, 8:                  id                             ...      0        0       0       0              0  , 10:                  id                             ...      0        0       0       0              0  , 11:                  id                             ...      0        0       0       0              0  , 13: id               object\ncomment_text     object\n...      int64\nidentity_hate     int64\ndtype: object, 15:                  id                             ...      0        0       0       0              0  , 17:                  id                             ...      0        0       0       0              0  }, 'TfidfVectorizer': <class 'sklearn.feature_extraction.text.TfidfVectorizer'>, '_':                  id                             ...      0        0       0       0              0  , '_10':                  id                             ...      0        0       0       0              0  , '_11':                  id                             ...      0        0       0       0              0  , '_13': id               object\ncomment_text     object\n...      int64\nidentity_hate     int64\ndtype: object, '_15':                  id                             ...      0        0       0       0              0  , '_17':                  id                             ...      0        0       0       0              0  , ...}\n        self.user_ns = {'In': ['', 'import numpy as np\\nimport pandas as pd\\nfrom skle...val_score\\nfrom sklearn.pipeline import make_union', \"train = pd.read_csv('/home/dan/Test/Kaggle/data/...an/Test/Kaggle/data/TF_idf/test.csv').fillna(' ')\", 'train[0]', 'train.head()', \"train = pd.read_csv('/home/dan/Test/Kaggle/data/...an/Test/Kaggle/data/TF_idf/test.csv').fillna(' ')\", 'train.head()', \"train['comment_text'].head()\", 'train.head()', \"train = pd.read_csv('/home/dan/Test/Kaggle/data/...an/Test/Kaggle/data/TF_idf/test.csv').fillna(' ')\", 'train.head()', 'train.head()', \"train = pd.read_csv('/home/dan/Test/Kaggle/data/...an/Test/Kaggle/data/TF_idf/test.csv').fillna(' ')\", 'train.dtypes', 'train.info()', 'train.head()', \"train = pd.read_csv('/home/dan/Test/Kaggle/data/...an/Test/Kaggle/data/TF_idf/test.csv').fillna(' ')\", 'train.head()', \"class_names = ['toxic', 'severe_toxic', 'obscene...t']\\nall_text = pd.concat([train_text, test_text])\", 'word_vectorizer = TfidfVectorizer(\\n    sublinear...union(word_vectorizer, char_vectorizer, n_jobs=2)', ...], 'LogisticRegression': <class 'sklearn.linear_model.logistic.LogisticRegression'>, 'Out': {4:                  id                             ...      0        0       0       0              0  , 6:                  id                             ...      0        0       0       0              0  , 7: 0    Explanation\\nWhy the edits made under my us...you remember...\nName: comment_text, dtype: object, 8:                  id                             ...      0        0       0       0              0  , 10:                  id                             ...      0        0       0       0              0  , 11:                  id                             ...      0        0       0       0              0  , 13: id               object\ncomment_text     object\n...      int64\nidentity_hate     int64\ndtype: object, 15:                  id                             ...      0        0       0       0              0  , 17:                  id                             ...      0        0       0       0              0  }, 'TfidfVectorizer': <class 'sklearn.feature_extraction.text.TfidfVectorizer'>, '_':                  id                             ...      0        0       0       0              0  , '_10':                  id                             ...      0        0       0       0              0  , '_11':                  id                             ...      0        0       0       0              0  , '_13': id               object\ncomment_text     object\n...      int64\nidentity_hate     int64\ndtype: object, '_15':                  id                             ...      0        0       0       0              0  , '_17':                  id                             ...      0        0       0       0              0  , ...}\n   2863             finally:\n   2864                 # Reset our crash handler in place\n   2865                 sys.excepthook = old_excepthook\n   2866         except SystemExit as e:\n\n...........................................................................\n/home/dan/Test/example/<ipython-input-20-2f1297c59012> in <module>()\n----> 1 vectorizer.fit(all_text)\n      2 train_features = vectorizer.transform(train_text)\n      3 test_features = vectorizer.transform(test_text)\n\n...........................................................................\n/home/dan/anaconda3/lib/python3.6/site-packages/sklearn/pipeline.py in fit(self=FeatureUnion(n_jobs=2,\n       transformer_list=[...abulary=None))],\n       transformer_weights=None), X=0         Explanation\\nWhy the edits made under ...Name: comment_text, Length: 312735, dtype: object, y=None)\n    709         \"\"\"\n    710         self.transformer_list = list(self.transformer_list)\n    711         self._validate_transformers()\n    712         transformers = Parallel(n_jobs=self.n_jobs)(\n    713             delayed(_fit_one_transformer)(trans, X, y)\n--> 714             for _, trans, _ in self._iter())\n        self._iter = <bound method FeatureUnion._iter of FeatureUnion...bulary=None))],\n       transformer_weights=None)>\n    715         self._update_transformer_list(transformers)\n    716         return self\n    717 \n    718     def fit_transform(self, X, y=None, **fit_params):\n\n...........................................................................\n/home/dan/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=Parallel(n_jobs=2), iterable=<generator object FeatureUnion.fit.<locals>.<genexpr>>)\n    784             if pre_dispatch == \"all\" or n_jobs == 1:\n    785                 # The iterable was consumed all at once by the above for loop.\n    786                 # No need to wait for async callbacks to trigger to\n    787                 # consumption.\n    788                 self._iterating = False\n--> 789             self.retrieve()\n        self.retrieve = <bound method Parallel.retrieve of Parallel(n_jobs=2)>\n    790             # Make sure that we get a last message telling us we are done\n    791             elapsed_time = time.time() - self._start_time\n    792             self._print('Done %3i out of %3i | elapsed: %s finished',\n    793                         (len(self._output), len(self._output),\n\n---------------------------------------------------------------------------\nSub-process traceback:\n---------------------------------------------------------------------------\nMemoryError                                        Wed Aug  8 13:35:29 2018\nPID: 24953                     Python 3.6.3: /home/dan/anaconda3/bin/python\n...........................................................................\n/home/dan/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        self.items = [(<function _fit_one_transformer>, (TfidfVectorizer(analyzer='char', binary=False, d...izer=None, use_idf=True,\n        vocabulary=None), 0         Explanation\\nWhy the edits made under ...Name: comment_text, Length: 312735, dtype: object, None), {})]\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\n/home/dan/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in <listcomp>(.0=<list_iterator object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        func = <function _fit_one_transformer>\n        args = (TfidfVectorizer(analyzer='char', binary=False, d...izer=None, use_idf=True,\n        vocabulary=None), 0         Explanation\\nWhy the edits made under ...Name: comment_text, Length: 312735, dtype: object, None)\n        kwargs = {}\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\n/home/dan/anaconda3/lib/python3.6/site-packages/sklearn/pipeline.py in _fit_one_transformer(transformer=TfidfVectorizer(analyzer='char', binary=False, d...izer=None, use_idf=True,\n        vocabulary=None), X=0         Explanation\\nWhy the edits made under ...Name: comment_text, Length: 312735, dtype: object, y=None)\n    562                         .format(list(kwargs.keys())[0]))\n    563     return Pipeline(_name_estimators(steps), memory=memory)\n    564 \n    565 \n    566 def _fit_one_transformer(transformer, X, y):\n--> 567     return transformer.fit(X, y)\n        transformer.fit = <bound method TfidfVectorizer.fit of TfidfVector...zer=None, use_idf=True,\n        vocabulary=None)>\n        X = 0         Explanation\\nWhy the edits made under ...Name: comment_text, Length: 312735, dtype: object\n        y = None\n    568 \n    569 \n    570 def _transform_one(transformer, weight, X):\n    571     res = transformer.transform(X)\n\n...........................................................................\n/home/dan/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py in fit(self=TfidfVectorizer(analyzer='char', binary=False, d...izer=None, use_idf=True,\n        vocabulary=None), raw_documents=0         Explanation\\nWhy the edits made under ...Name: comment_text, Length: 312735, dtype: object, y=None)\n   1356 \n   1357         Returns\n   1358         -------\n   1359         self : TfidfVectorizer\n   1360         \"\"\"\n-> 1361         X = super(TfidfVectorizer, self).fit_transform(raw_documents)\n        X = undefined\n        self.fit_transform = <bound method TfidfVectorizer.fit_transform of T...zer=None, use_idf=True,\n        vocabulary=None)>\n        raw_documents = 0         Explanation\\nWhy the edits made under ...Name: comment_text, Length: 312735, dtype: object\n   1362         self._tfidf.fit(X)\n   1363         return self\n   1364 \n   1365     def fit_transform(self, raw_documents, y=None):\n\n...........................................................................\n/home/dan/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py in fit_transform(self=TfidfVectorizer(analyzer='char', binary=False, d...izer=None, use_idf=True,\n        vocabulary=None), raw_documents=0         Explanation\\nWhy the edits made under ...Name: comment_text, Length: 312735, dtype: object, y=None)\n    870 \n    871         if self.binary:\n    872             X.data.fill(1)\n    873 \n    874         if not self.fixed_vocabulary_:\n--> 875             X = self._sort_features(X, vocabulary)\n        X = <312735x1181355 sparse matrix of type '<class 'n... stored elements in Compressed Sparse Row format>\n        self._sort_features = <bound method CountVectorizer._sort_features of ...zer=None, use_idf=True,\n        vocabulary=None)>\n        vocabulary = {'\\x02': 0, '\\x02 ': 1, '\\x02 n': 2, '\\x02 ni': 3, '\\n': 4, '\\n!': 5, '\\n! ': 6, '\\n! !': 7, '\\n! a': 8, '\\n! b': 9, ...}\n    876 \n    877             n_doc = X.shape[0]\n    878             max_doc_count = (max_df\n    879                              if isinstance(max_df, numbers.Integral)\n\n...........................................................................\n/home/dan/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py in _sort_features(self=TfidfVectorizer(analyzer='char', binary=False, d...izer=None, use_idf=True,\n        vocabulary=None), X=<312735x1181355 sparse matrix of type '<class 'n... stored elements in Compressed Sparse Row format>, vocabulary={'\\x02': 0, '\\x02 ': 1, '\\x02 n': 2, '\\x02 ni': 3, '\\n': 4, '\\n!': 5, '\\n! ': 6, '\\n! !': 7, '\\n! a': 8, '\\n! b': 9, ...})\n    726         map_index = np.empty(len(sorted_features), dtype=np.int32)\n    727         for new_val, (term, old_val) in enumerate(sorted_features):\n    728             vocabulary[term] = new_val\n    729             map_index[old_val] = new_val\n    730 \n--> 731         X.indices = map_index.take(X.indices, mode='clip')\n        X.indices = array([     0,      1,      2, ..., 220672, 393629, 437212], dtype=int32)\n        map_index.take = <built-in method take of numpy.ndarray object>\n    732         return X\n    733 \n    734     def _limit_features(self, X, vocabulary, high=None, low=None,\n    735                         limit=None):\n\nMemoryError: \n___________________________________________________________________________"
     ]
    }
   ],
   "source": [
    "vectorizer.fit(all_text)\n",
    "train_features = vectorizer.transform(train_text)\n",
    "test_features = vectorizer.transform(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
